{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebbb704b-83ca-4707-b931-94dc08595fa3",
   "metadata": {},
   "source": [
    "# AssQ 24-Apr Dimentionality Reduction-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ecbad2-2db9-4a54-aa7d-4ed8ef1c2a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436df575-4c22-427d-bb5c-787e51ba884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "This paper introduces a Projected Principal Component Analysis (Projected-PCA), which employees\n",
    "principal component analysis to the projected (smoothed) data matrix onto a given linear space spanned by covariates.\n",
    "When it applies to high-dimensional factor analysis, the projection removes noise components.\n",
    "\n",
    "The steps to perform PCA are as follows.\n",
    "Compute the covariance matrix. ...\n",
    "Find eigenvectors ( ) and eigenvalues ( ) of the covariance matrix using singular value decomposition. ...\n",
    "Select first columns from eigenvector matrix. ...\n",
    "Compute projections of original observation onto new vector form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e20d9c-5e28-4e1f-80f9-0cf9ffca0c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741881c4-534f-4c37-83df-e62da1f96440",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f5e41-f78a-4548-b6ea-0773afd7b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "a classic perspective is that PCA finds a set of directions (technically, \n",
    "a linear subspace) that maximizes the variance of the data once it is projected into that space.\n",
    "\n",
    "PCA can be used to reduce the dimensionality of the data by creating a set of derived \n",
    "variables that are linear combinations of the original variables.\n",
    "The values of the derived variables are given in the columns of the scores matrix Z.\n",
    "\n",
    "PCA seeks to solve a sequence of optimization problems. The first in the sequence is the unconstrained problem maximizeuTSuuTu,u∈Rp. \n",
    "Since uTu=‖u‖22=‖u‖‖u‖, the above unconstrained problem is equivalent to the constrained problem maximizeuTSusubject touTu=1.\n",
    "\n",
    "For any q, setting the columns of V to the loadings for the first q PCs and Z to the scores on \n",
    "the corresponding subspace gives an optimal solution for (3). \n",
    "Therefore, PCA provides optimal solutions to a series of optimization problems simultaneously: it solves (3) for q = 1,...,m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425bd60-40d9-4e43-8652-79b05d87f737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686bc4a0-0465-486e-a888-69eca2ee1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24f5ca-18f0-423c-a005-217cdacfb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA is simply described as “diagonalizing the covariance matrix”.\n",
    "What does diagonalizing a matrix mean in this context? It simply means that we need to \n",
    "find a non-trivial linear combination of our original variables such that the covariance matrix is diagonal.\n",
    "\n",
    "\n",
    "Covariance-based PCA is equivalent to MLPCA whenever the variance-covariance matrix of the measurement\n",
    "errors is assumed diagonal with equal elements on its diagonal. \n",
    "The measurement error variance parameter can then be estimated by \n",
    "applying the probabilistic principal component analysis (PPCA) model.\n",
    "\n",
    "The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) \n",
    "that has as entries the covariances associated\n",
    "with all possible pairs of the initial variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0614e9-24d5-4077-aa96-070597ac6920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d57fa-d95f-4f98-8df2-38c8ac629d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16735297-5715-466f-a465-6c178b341a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "If our sole intention of doing PCA is for data visualization, the best number of components is 2 or 3.\n",
    "If we really want to reduce the size of the dataset,\n",
    "the best number of principal components is much less than the number of variables in the original dataset.\n",
    "\n",
    "the idea is 10-dimensional data gives you 10 principal components, but PCA tries \n",
    "to put maximum possible information in the first component,\n",
    "then maximum remaining information in the second and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460eeca4-9961-4feb-ac24-37b0565b2a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f8b8b-c6c3-4934-a7c6-5ae00c31b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d7e59f-9bde-4299-89c0-ef51413feb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA, generally called data reduction technique, is very useful feature selection technique as\n",
    "it uses linear algebra to transform the dataset into a compressed form. \n",
    "We can implement PCA feature selection technique with the help of PCA class of scikit-learn Python library.\n",
    "\n",
    "Principal Component Analysis (PCA) is a popular linear feature extractor used for unsupervised feature selection based \n",
    "on eigenvectors analysis to identify critical original features for principal component.\n",
    "\n",
    "In many datasets we find that number of features are very large and if we want to train the model\n",
    "it take more computational cost. To decrease the number of features we can use Principal component analysis (PCA). \n",
    "PCA decrease the number of features by selecting dimension of features which have most of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db7103-dea3-4c7a-96d8-8fb0161a10b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54cf5a-d854-44bf-831a-8dd9906288a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3343d2b6-eca2-47eb-a5b8-31f902ea29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Applications of PCA in Machine Learning\n",
    "PCA is used to visualize multidimensional data.\n",
    "It is used to reduce the number of dimensions in healthcare data.\n",
    "PCA can help resize an image.\n",
    "It can be used in finance to analyze stock data and forecast returns.\n",
    "PCA helps to find patterns in the high-dimensional datasets.\n",
    "\n",
    "PCA forms the basis of multivariate data analysis based on projection methods.\n",
    "The most important use of PCA is to represent a multivariate data table as smaller\n",
    "set of variables (summary indices) in order to observe trends, jumps, clusters and outliers.\n",
    "\n",
    "Principal components are independent of each other, so removes correlated features. \n",
    "PCA improves the performance of the ML algorithm as it eliminates correlated variables \n",
    "that don't contribute in any decision making.\n",
    "PCA helps in overcoming data overfitting issues by decreasing the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315fada-df2a-4fd8-9242-44c2d674f8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ec238-f890-4e78-9c3a-6ff12c33078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7.What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fee608-18a7-415b-aba0-1534741260ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "The variance explained can be understood as the ratio of the vertical spread of the regression line\n",
    "(i.e., from the lowest point on the line to the highest point on the line) to the vertical spread of the data \n",
    "(i.e., from the lowest data point to the highest data point).\n",
    "\n",
    "PCA is defined as an orthogonal linear transformation that transforms the data to a \n",
    "new coordinate system such that the greatest variance by some scalar projection of \n",
    "the data comes to lie on the first coordinate (called the first principal component), \n",
    "the second greatest variance on the second coordinate, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c864212-df52-45b5-bdef-43765aa00dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131081e-c655-47c4-8912-5d28c289829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb9223-8898-44a4-807f-0419417c752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA works by finding the directions of maximum variance in the data\n",
    "set and projecting the data onto these directions. The principal components \n",
    "ordered by the amount of variance they explain and are used for feature selection,\n",
    "data compression, clustering, and classification.\n",
    "\n",
    "PCA is an unsupervised method2. It searches for the directions that data have the largest variance3. \n",
    "Maximum number of principal components <= number of features4.\n",
    "\n",
    "In order to find the direction of maximum variance, \n",
    "fit a straight line in the data set where the data is most spread out.\n",
    "A vertical line with the data points projected on to it will look like the diagram show below.\n",
    "It looks like the line doesn't show the direction of maximum variance in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8aca9-8d8e-49f1-a403-42ef8e0b842d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca6404-646a-4709-bdf9-08c2a4f224af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abfa639-de0a-4b5d-8d35-0d31a57d2c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA generally tries to find the lower-dimensional surface to project the high-dimensional data.\n",
    "PCA works by considering the variance of each attribute because the high attribute shows the good split between the classes,\n",
    "and hence it reduces the dimensionality.\n",
    "\n",
    "PCA is a linear dimension-reduction technique that finds new axes that maximize the variance in the data.\n",
    "The first of these principal axes maximizes the most variance, followed by the second, and the third, \n",
    "and so on, which are all orthogonal to the previously computed axes.\n",
    "\n",
    "PCA can be used for projecting and visualizing data in lower dimensions.\n",
    "Explanation: Sometimes it is very useful to plot the data in lower dimensions.\n",
    "We can take the first 2 principal components and then use visualization of the data using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c11cd-7ce2-4003-b945-71f1c16c4636",
   "metadata": {},
   "outputs": [],
   "source": [
    "..............................The End......................."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
